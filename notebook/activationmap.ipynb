{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import skimage.transform\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "import yaml\n",
    "config_filename = 'ResNeSt003.yaml'\n",
    "with open(f\"../configs/{config_filename}\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  recording_id  species_id  songtype_id    t_min     f_min    t_max     f_max  \\\n",
       "0    003bec244          14            1  44.5440  2531.250  45.1307   5531.25   \n",
       "1    006ab765f          23            1  39.9615  7235.160  46.0452  11283.40   \n",
       "2    007f87ba2          12            1  39.1360   562.500  42.2720   3281.25   \n",
       "3    0099c367b          17            4  51.4206  1464.260  55.1996   4565.04   \n",
       "4    009b760e6          10            1  50.0854   947.461  52.5293  10852.70   \n",
       "\n",
       "  data_type  \n",
       "0        tp  \n",
       "1        tp  \n",
       "2        tp  \n",
       "3        tp  \n",
       "4        tp  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>recording_id</th>\n      <th>species_id</th>\n      <th>songtype_id</th>\n      <th>t_min</th>\n      <th>f_min</th>\n      <th>t_max</th>\n      <th>f_max</th>\n      <th>data_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>003bec244</td>\n      <td>14</td>\n      <td>1</td>\n      <td>44.5440</td>\n      <td>2531.250</td>\n      <td>45.1307</td>\n      <td>5531.25</td>\n      <td>tp</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>006ab765f</td>\n      <td>23</td>\n      <td>1</td>\n      <td>39.9615</td>\n      <td>7235.160</td>\n      <td>46.0452</td>\n      <td>11283.40</td>\n      <td>tp</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>007f87ba2</td>\n      <td>12</td>\n      <td>1</td>\n      <td>39.1360</td>\n      <td>562.500</td>\n      <td>42.2720</td>\n      <td>3281.25</td>\n      <td>tp</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0099c367b</td>\n      <td>17</td>\n      <td>4</td>\n      <td>51.4206</td>\n      <td>1464.260</td>\n      <td>55.1996</td>\n      <td>4565.04</td>\n      <td>tp</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>009b760e6</td>\n      <td>10</td>\n      <td>1</td>\n      <td>50.0854</td>\n      <td>947.461</td>\n      <td>52.5293</td>\n      <td>10852.70</td>\n      <td>tp</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "audio_path = Path('../input/rfcx-species-audio-detection/train/')\n",
    "train_tp = pd.read_csv('../input/rfcx-species-audio-detection/train_tp.csv')\n",
    "train_fp = pd.read_csv('../input/rfcx-species-audio-detection/train_fp.csv')\n",
    "train_tp['data_type'] = 'tp'\n",
    "train_fp['data_type'] = 'fp'\n",
    "train = pd.concat([train_tp, train_fp])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def get_spec_sample(df, total_time=60, PERIOD=10):\n",
    "    sample = df.sample(1)\n",
    "    recording_id = sample[\"recording_id\"].values[0]\n",
    "    main_species_id = sample[\"species_id\"].values[0]\n",
    "\n",
    "    y, sr = sf.read(audio_path / f\"{recording_id}.flac\")  # for default\n",
    "\n",
    "    # データの長さを全てtotal_time分にする\n",
    "    len_y = len(y)\n",
    "    total_length = total_time * sr\n",
    "    if len_y < total_length:\n",
    "        new_y = np.zeros(total_length, dtype=y.dtype)\n",
    "        start = np.random.randint(total_length - len_y)\n",
    "        new_y[start:start + len_y] = y\n",
    "        y = new_y.astype(np.float32)\n",
    "    elif len_y > total_length:\n",
    "        start = np.random.randint(len_y - total_length)\n",
    "        y = y[start:start + total_length].astype(np.float32)\n",
    "    else:\n",
    "        y = y.astype(np.float32)\n",
    "\n",
    "    # PERIODO単位に分割(現在は6等分)\n",
    "    split_y = np.split(y, total_time/PERIOD)\n",
    "\n",
    "    images = []\n",
    "    original_images = []\n",
    "    # 分割した音声を一つずつ画像化してリストで返す\n",
    "    for y in split_y:\n",
    "        melspec = librosa.feature.melspectrogram(y, sr=sr, n_mels=128, fmin=80, fmax=15000, power=1.5)\n",
    "        melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "        original_images.append(melspec)\n",
    "        image = mono_to_color(melspec)\n",
    "        image = cv2.resize(image, (400, 224))\n",
    "        image = np.moveaxis(image, 2, 0)\n",
    "        image = (image / 255.0).astype(np.float32)\n",
    "        images.append(image)\n",
    "\n",
    "    labels = np.zeros(len(df['species_id'].unique()), dtype=np.float32)\n",
    "    labels[main_species_id] = 1.0\n",
    "    return original_images, images, labels\n",
    "\n",
    "\n",
    "def mono_to_color(X: np.ndarray,\n",
    "                  mean=None,\n",
    "                  std=None,\n",
    "                  norm_max=None,\n",
    "                  norm_min=None,\n",
    "                  eps=1e-6):\n",
    "    # Stack X as [X,X,X]\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    X = X - mean\n",
    "    std = std or X.std()\n",
    "    Xstd = X / (std + eps)\n",
    "    _min, _max = Xstd.min(), Xstd.max()\n",
    "    norm_max = norm_max or _max\n",
    "    norm_min = norm_min or _min\n",
    "    if (_max - _min) > eps:\n",
    "        # Normalize to [0, 255]\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images, images, labels = get_spec_sample(train_tp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting torchlibrosa\n",
      "  Downloading torchlibrosa-0.0.5-py3-none-any.whl (8.9 kB)\n",
      "Installing collected packages: torchlibrosa\n",
      "Successfully installed torchlibrosa-0.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install torchlibrosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: \"https://s3.us-west-1.wasabisys.com/resnest/torch/resnest50-528c19ca.pth\" to /root/.cache/torch/hub/checkpoints/resnest50-528c19ca.pth\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0.00/105M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8927c139bd345cc8e647cee642b865b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../output/1226_162624/ResNeSt50-0.ckpt'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8f0a36b9936b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf'{model_name}-{fold}-v0.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO foldごとのモデルを取得できるようにする\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../output/1226_162624/ResNeSt50-0-v0.ckpt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8f0a36b9936b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf'{model_name}-{fold}-v0.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO foldごとのモデルを取得できるようにする\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf'{model_name}-{fold}.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO foldごとのモデルを取得できるようにする\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../output/1226_162624/ResNeSt50-0.ckpt'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.models import get_model\n",
    "\n",
    "fold = 0\n",
    "output_dir = Path('../output/1226_162624/')\n",
    "model_name = 'ResNeSt50'\n",
    "model = get_model(config, fold)\n",
    "try:\n",
    "    ckpt = torch.load(output_dir / f'{model_name}-{fold}-v0.ckpt')  # TODO foldごとのモデルを取得できるようにする\n",
    "except:\n",
    "    ckpt = torch.load(output_dir / f'{model_name}-{fold}.ckpt')  # TODO foldごとのモデルを取得できるようにする\n",
    "model.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features from a model\n",
    "class SaveFeatures():\n",
    "    features = None\n",
    "    def __init__(self, module): \n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "\n",
    "    def hook_fn(self, module, input, output): \n",
    "        self.features = output.data.numpy()\n",
    "\n",
    "    def remove(self): \n",
    "        self.hook.remove()\n",
    "\n",
    "def getCAM(feature_conv, weight_fc, class_idx):\n",
    "    _, nc, h, w = feature_conv.shape\n",
    "    cam = weight_fc[class_idx].dot(feature_conv.reshape((nc, h * w)))\n",
    "    cam = cam.reshape(h, w)\n",
    "    cam = cam - np.min(cam)\n",
    "    cam_img = cam / np.max(cam)\n",
    "    return [cam_img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features from last conv layer\n",
    "final_layer = model.model._modules.get('layer4')\n",
    "activated_features = SaveFeatures(final_layer)\n",
    "\n",
    "# Inference\n",
    "_ = model.eval()\n",
    "input = torch.Tensor(images[0])\n",
    "input = input.unsqueeze(0)\n",
    "prediction = model(input)\n",
    "pred_probabilities = F.softmax(prediction).data.squeeze()\n",
    "activated_features.remove()\n",
    "print('Top-1 prediction:', torch.topk(pred_probabilities, 1)[0])\n",
    "\n",
    "# Take weights from the first linear layer\n",
    "weight_softmax_params = list(model.model._modules.get('fc').parameters())\n",
    "weight_softmax = np.squeeze(weight_softmax_params[0].data.numpy())\n",
    "\n",
    "# Get the top-1 prediction and get CAM\n",
    "class_idx = torch.topk(pred_probabilities, 1)[1].int()\n",
    "print(class_idx)\n",
    "print(activated_features.features.shape)\n",
    "print(weight_softmax[class_idx])\n",
    "overlay = getCAM(activated_features.features, weight_softmax, class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax1.imshow(original_images[0])\n",
    "\n",
    "image = original_images[0]\n",
    "ax2.imshow(image)\n",
    "ax2.imshow(skimage.transform.resize(overlay[0], (image.shape[0], image.shape[1])), alpha=0.5, cmap='jet');\n",
    "plt.show()"
   ]
  }
 ]
}